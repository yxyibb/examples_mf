Overfitting 也被称为过度学习，过度拟合。
分类问题（classification）：很精确的区分了所有的训练数据，但是并没有描述数据的整体特征，对新测试数据的适应性较差
回归问题（regression）：尽管它经过了所有的训练点，但是不能很好的反应数据的趋势，预测能力严重不足

Tensorflow提供dropout方法解决overfitting问题
例子使用sklearn数据库中的数据
安装 Scikit-learn (sklearn) 最简单的方法就是使用 pip 安装它.
首先确认自己电脑中有安装
    Python (>=2.6 或 >=3.3 版本)
    Numpy (>=1.6.1)
    Scipy (>=0.9)
Terminal:输入 pip install -U scikit-learn

keep_prob = tf.placeholder(tf.float32)
...
...
Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
keep_prob是保留概率，即我们要保留的结果所占比例，它作为一个placeholder，在run时传入， 当keep_prob=1的时候，相当于100%保留，也就是dropout没有起作用

from __future__ import print_function
import tensorflow as tf
from sklearn.datasets import load_digits
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import LabelBinarizer

#data
digits = load_digits()
X = digits.data
y = digits.target
y = LabelBinarizer().fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)

def add_layer(inputs, in_size, out_size, layer_name, activation_function = None):
    weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs,weights) + biases
    #dropout
    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    tf.summary.histogram(layer_name + '/outputs', outputs)
    return outputs

#placeholder
keep_prob = tf.placeholder(tf.float32)
xs = tf.placeholder(tf.float32,[None,64]) #8*8
ys = tf.placeholder(tf.float32,[None,10])
#NN
l1 = add_layer(xs, 64, 50,'l1',activation_function=tf.nn.tanh)
prediction = add_layer(l1, 50, 10,'l2', activation_function=tf.nn.softmax)
cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))
tf.summary.scalar('loss',cross_entropy)
train_step = tf.train.GradientDescentOptimizer(0.6).minimize(cross_entropy)
#run
sess = tf.Session()
merged = tf.summary.merge_all()
#summary writer goes in here
train_writer = tf.summary.FileWriter('logs/train',sess.graph)
test_writer = tf.summary.FileWriter('logs/test',sess.graph)

if int((tf.__version__).split('.')[1])<12 and int((tf.__version__).split('.')[0])<1:
    init = tf.initialize_all_variables()
else:
    init = tf.global_variables_initializer()
sess.run(init)
for i in range(500):
    sess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob:0.5})
    if i % 50 == 0:
        train_result = sess.run(merged, feed_dict={xs: X_train, ys:y_train,keep_prob:1})
        test_result = sess.run(merged, feed_dict={xs:X_test, ys:y_test, keep_prob:1})
        train_writer.add_summary(train_result,i)
        test_writer.add_summary(test_result,i)
