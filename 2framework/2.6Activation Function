激励函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经系统。激励函数的实质是非线性方程。
Tensorflow 的神经网络 里面处理较为复杂的问题时都会需要运用激励函数 activation function.

为什么要激励函数 (Activation Function)
激励函数是”掰弯利器”, 套在了原函数上 用力一扭, 原来的 Wx 结果就被扭弯了.

其实它就是另外一个非线性函数. 比如说relu, sigmoid, tanh。将这些掰弯利器嵌套在原有的结果之上, 强行把原有的线性结果给扭曲了，使得输出结果 y 也有了非线性的特征。
举个例子, 比如我使用了 relu 这个掰弯利器, 如果此时 Wx 的结果是1, y 还将是1, 不过 Wx 为-1的时候, y 不再是-1, 而会是0.

你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候，只有这些可微分的激励函数才能把误差传递回去。
在少量层结构中, 我们可以尝试很多种不同的激励函数。
在卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 relu。
在循环神经网络中 recurrent neural networks, 推荐的是 tanh 或者是 relu 
